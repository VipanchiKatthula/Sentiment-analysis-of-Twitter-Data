{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1= open(r\"E:\\MS_Studies\\sem2\\566\\hw2\\train.txt\",\"rt\", encoding=\"utf8\")\n",
    "train1=file1.read().lower()\n",
    "file2=open(r\"E:\\MS_Studies\\sem2\\566\\hw2\\test.txt\",\"rt\", encoding=\"utf8\")\n",
    "test1=file2.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting text to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train1.splitlines()\n",
    "y=[]\n",
    "for i in train:\n",
    "    y.append(i.split(\",\",3))\n",
    "train_df=pd.DataFrame(y) \n",
    "train_df=train_df[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test1.splitlines()\n",
    "y=[]\n",
    "for i in test:\n",
    "    y.append(i.split(\",\",3))\n",
    "test_df=pd.DataFrame(y) \n",
    "test_df=test_df[[1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step a: function to remove punctuation/apostrophe\n",
    "def removal(textfile):\n",
    "    y=textfile[3]\n",
    "    ps =SnowballStemmer(\"english\")\n",
    "    cleaned=[]\n",
    "    for i in y:\n",
    "        i.replace(\"'\", \" \")  #step 1: replacing apostraphe\n",
    "        cleaned.append(i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))) # step 2: remove_punctuation             \n",
    "    \n",
    "    # step3: removing stop words and perform stemming\n",
    "    output=[]\n",
    "    for i in cleaned:\n",
    "        output.append(\" \".join([ps.stem(w) for w in i.split()  if not w in list_stop_words and w.isalpha()]))    \n",
    "        \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned=pd.DataFrame(train_df[1])\n",
    "train_cleaned.insert(1, 2, removal(train_df))\n",
    "train_cleaned=train_cleaned.drop(train_cleaned.index[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned=pd.DataFrame(test_df[1])\n",
    "test_cleaned.insert(1, 2, removal(test_df))\n",
    "test_cleaned=test_cleaned.drop(test_cleaned.index[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF/IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_train = vectorizer.fit_transform(train_cleaned[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vectorizer.transform(test_cleaned[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<90000x72174 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 651487 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x72174 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 64078 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
