{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1582491677031,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "Z9JrNZeueDQF",
    "outputId": "45085783-5544-4b71-d625-54a3fb89a932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8EOEpYUeDQM"
   },
   "outputs": [],
   "source": [
    "list_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20031,
     "status": "ok",
     "timestamp": 1582491706741,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "c9HkOJkxeFUY",
    "outputId": "d8f05307-60ec-4fe3-bdae-dfe427aeb275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wCOlGuReDQP"
   },
   "outputs": [],
   "source": [
    "file1= open(r\"/content/drive/My Drive/Colab Notebooks/train.txt\",\"rt\", encoding=\"utf8\")\n",
    "train1=file1.read().lower()\n",
    "file2=open(r\"/content/drive/My Drive/Colab Notebooks/test.txt\",\"rt\", encoding=\"utf8\")\n",
    "test1=file2.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qik4JWdHeDQS"
   },
   "source": [
    "#### Converting text to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJFfwBwDeDQT"
   },
   "outputs": [],
   "source": [
    "train=train1.splitlines()\n",
    "y=[]\n",
    "for i in train:\n",
    "    y.append(i.split(\",\",3))\n",
    "train_df=pd.DataFrame(y) \n",
    "train_df=train_df[[1,3]]\n",
    "train_df[3]=train_df[3].str.lstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHflOV8ZeDQW"
   },
   "outputs": [],
   "source": [
    "test=test1.splitlines()\n",
    "y=[]\n",
    "for i in test:\n",
    "    y.append(i.split(\",\",3))\n",
    "test_df=pd.DataFrame(y) \n",
    "test_df=test_df[[1,3]]\n",
    "test_df[3]=test_df[3].str.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0uRtNfzeDQY"
   },
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b96322mMeDQZ"
   },
   "outputs": [],
   "source": [
    "# step a: function to remove punctuation/apostrophe\n",
    "def removal(textfile):\n",
    "    y=textfile[3]\n",
    "    ps =SnowballStemmer(\"english\")\n",
    "    cleaned=[]\n",
    "    for i in y:\n",
    "        i.replace(\"'\", \" \")  #step 1: replacing apostraphe\n",
    "        i=re.sub(r\"\\@\\w+\",\" \",i)   #step2: removing words starting with @\n",
    "        cleaned.append(i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))) # step 3: remove_punctuation             \n",
    "    \n",
    "    # step4: removing stop words and perform stemming\n",
    "    output=[]\n",
    "    for i in cleaned:\n",
    "        output.append(\" \".join([ps.stem(w) for w in i.split()  if  w not in list_stop_words and w.isalpha()]))    \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGYTfTIKeDQc"
   },
   "outputs": [],
   "source": [
    "train_cleaned=pd.DataFrame(train_df[1])\n",
    "train_cleaned.insert(1, 2, removal(train_df))\n",
    "train_cleaned=train_cleaned.drop(train_cleaned.index[[0]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Dz7Wb5veDQe"
   },
   "outputs": [],
   "source": [
    "test_cleaned=pd.DataFrame(test_df[1])\n",
    "test_cleaned.insert(1, 2, removal(test_df))\n",
    "test_cleaned=test_cleaned.drop(test_cleaned.index[[0]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEIo04faeDQg"
   },
   "source": [
    "#### TF/IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LH8GNGNmeDQh"
   },
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(max_features=10000)#max_df=0.90, min_df=5)\n",
    "tfidf_train = vectorizer.fit_transform(train_cleaned[2])\n",
    "tfidf_test = vectorizer.transform(test_cleaned[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWGd28EziFWt"
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_train=pd.DataFrame(tfidf_train.todense(),columns=vectorizer.get_feature_names())\n",
    "tfidf_test=pd.DataFrame(tfidf_test.todense(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Fz7E6c6zAQK"
   },
   "outputs": [],
   "source": [
    "# Merge the sentiment column from the original training data and the calculated TF-IDF matrix\n",
    "tfidf_train.insert(0,\"target_var\",pd.to_numeric(train_cleaned[1]))\n",
    "tfidf_test.insert(0,\"target_var\",pd.to_numeric(test_cleaned[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAqX3OAAeDQs"
   },
   "outputs": [],
   "source": [
    "# Split training features and labels\n",
    "traindata = tfidf_train.iloc[:, 1:]\n",
    "trainlabel = pd.DataFrame(tfidf_train.iloc[:, 0])\n",
    "\n",
    "# Split testing features and labels\n",
    "testdata = tfidf_test.iloc[:, 1:]\n",
    "testlabel = pd.DataFrame(tfidf_test.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1UTJCBL3Xpr"
   },
   "outputs": [],
   "source": [
    "# Class for logistic regression\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        # Initialize learning rate and weights\n",
    "        self.lr = 0.1\n",
    "        self.theta = np.zeros(shape=(tfidf_train.shape[1]-1, 1))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "       \n",
    "      \n",
    "        # Iterate and learn the parameters\n",
    "        for i in range(1000):\n",
    "            z = np.dot(X, self.theta)\n",
    "            y_pred = self.sigmoid(z)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            gradient = np.dot(X.T, (y_pred - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "\n",
    "    # Get the probabilities for predictions\n",
    "    def predict_prob(self, X):\n",
    "       # intercept = np.ones((X.shape[0], 1))\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    # Determine the label based on probabilities\n",
    "    def predict(self, X):\n",
    "        predictedLabels = np.array([])\n",
    "        pred_probs = self.predict_prob(X)\n",
    "\n",
    "        for prob in pred_probs:\n",
    "            # The avg probability for 0 and 1 in the training data is around 0.5 so we set the threshold for classification at this mean\n",
    "            if prob >= 0.55:\n",
    "                predictedLabels = np.append(predictedLabels, 1)\n",
    "            else:\n",
    "                predictedLabels = np.append(predictedLabels, 0)\n",
    "\n",
    "        return predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPmuD8vY3kWJ"
   },
   "outputs": [],
   "source": [
    "# Array for storing accuracy scores\n",
    "scores = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1582491838149,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "fl__-antgboU",
    "outputId": "e350e27c-bbba-4ffc-d3c9-4c7d76d68057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7O5R49_3pbP"
   },
   "outputs": [],
   "source": [
    "# Initialize 10-fold cross validation\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Iterate over folds and call the classifier\n",
    "for train_index, test_index in kf.split(traindata):\n",
    "    x_train, x_test = traindata.iloc[train_index], traindata.iloc[test_index]\n",
    "    y_train, y_test = trainlabel.iloc[train_index], trainlabel.iloc[test_index]\n",
    "    logReg = LogisticRegression()\n",
    "    logReg.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "    pred = logReg.predict(x_test)\n",
    "\n",
    "    scores = np.append(scores, accuracy_score(y_test, pred))\n",
    "\n",
    "print(\"Accuracy over 10-fold cross validations are: \", scores)\n",
    "print(\"Mean accuracy: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1582495055180,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "JMHiW721O0k9",
    "outputId": "f755834f-2ce5-4335-df2c-e535ae71539d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5994"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log=LogisticRegression()\n",
    "test_predicted = log.predict(testdata)\n",
    "#Accuracy on test data\n",
    "accuracy_test = accuracy_score(testlabel, test_predicted)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1582495071643,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "TSLotRfVO87y",
    "outputId": "f50080fc-7a67-4ff1-d1ad-fa7632b5d8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1\n",
      "0  0  4006\n",
      "1  0  5994\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix\n",
    "confusionMatrix = pd.DataFrame(data = confusion_matrix(testlabel, (test_predicted >= 0.5) .astype(int)), columns=[\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1582495078419,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "XyS9dmtUPBOM",
    "outputId": "5a0085ee-2b1c-4f00-de69-94abd283b587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  59.94\n"
     ]
    }
   ],
   "source": [
    "# Precision = TP/(TP + FP)\n",
    "precision = round((confusionMatrix.iloc[1, 1] / (confusionMatrix.iloc[1, 1] + confusionMatrix.iloc[0, 1])) * 100, 2)\n",
    "print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1582495080872,
     "user": {
      "displayName": "Sruthi Reddy Narapareddy",
      "photoUrl": "",
      "userId": "12929992835128566674"
     },
     "user_tz": 360
    },
    "id": "6NNgSv57PCcL",
    "outputId": "85b11e9c-158a-43ed-f33f-4eaeaf16842f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  100.0\n"
     ]
    }
   ],
   "source": [
    "# Recall = TP/(TP + FN)\n",
    "recall = round((confusionMatrix.iloc[1, 1] / (confusionMatrix.iloc[1, 1] + confusionMatrix.iloc[1, 0])) * 100, 2)\n",
    "print(\"Recall: \", recall)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "566_hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
