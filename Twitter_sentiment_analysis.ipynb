{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "566_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sruthi1014/Sentimental-analysis-of-Twitter-Data/blob/master/Twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9JrNZeueDQF",
        "colab_type": "code",
        "outputId": "5d3ae58a-08e2-4f37-cfad-f20b9e6589a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8EOEpYUeDQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9HkOJkxeFUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q xlrd \n",
        "!git clone https://github.com/sruthi1014/Sentimental-analysis-of-Twitter-Data.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCOlGuReDQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1= open(r\"Sentimental-analysis-of-Twitter-Data/train.txt\",\"rt\", encoding=\"utf8\")\n",
        "train1=file1.read().lower()\n",
        "file2=open(r\"Sentimental-analysis-of-Twitter-Data/test.txt\",\"rt\", encoding=\"utf8\")\n",
        "test1=file2.read().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qik4JWdHeDQS",
        "colab_type": "text"
      },
      "source": [
        "#### Converting text to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJFfwBwDeDQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=train1.splitlines()\n",
        "y=[]\n",
        "for i in train:\n",
        "    y.append(i.split(\",\",3))\n",
        "train_df=pd.DataFrame(y) \n",
        "train_df=train_df[[1,3]]\n",
        "train_df[3]=train_df[3].str.lstrip()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHflOV8ZeDQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=test1.splitlines()\n",
        "y=[]\n",
        "for i in test:\n",
        "    y.append(i.split(\",\",3))\n",
        "test_df=pd.DataFrame(y) \n",
        "test_df=test_df[[1,3]]\n",
        "test_df[3]=test_df[3].str.lstrip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0uRtNfzeDQY",
        "colab_type": "text"
      },
      "source": [
        "#### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96322mMeDQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step a: function to remove punctuation/apostrophe\n",
        "def removal(textfile):\n",
        "    y=textfile[3]\n",
        "    ps =SnowballStemmer(\"english\")\n",
        "    cleaned=[]\n",
        "    for i in y:\n",
        "        i.replace(\"'\", \" \")  #step 1: replacing apostraphe\n",
        "        i=re.sub(r\"\\@\\w+\",\" \",i)   #step2: removing words starting with @\n",
        "        cleaned.append(i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))) # step 3: remove_punctuation             \n",
        "    \n",
        "    # step4: removing stop words and perform stemming\n",
        "    output=[]\n",
        "    for i in cleaned:\n",
        "        output.append(\" \".join([ps.stem(w) for w in i.split()  if  w not in list_stop_words and w.isalpha()]))    \n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYTfTIKeDQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cleaned=pd.DataFrame(train_df[1])\n",
        "train_cleaned.insert(1, 2, removal(train_df))\n",
        "train_cleaned=train_cleaned.drop(train_cleaned.index[[0]]).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dz7Wb5veDQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cleaned=pd.DataFrame(test_df[1])\n",
        "test_cleaned.insert(1, 2, removal(test_df))\n",
        "test_cleaned=test_cleaned.drop(test_cleaned.index[[0]]).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEIo04faeDQg",
        "colab_type": "text"
      },
      "source": [
        "#### TF/IDF calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH8GNGNmeDQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer=TfidfVectorizer(max_features=10000)#max_df=0.90, min_df=5)\n",
        "tfidf_train = vectorizer.fit_transform(train_cleaned[2])\n",
        "tfidf_test = vectorizer.transform(test_cleaned[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWGd28EziFWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tfidf_train=pd.DataFrame(tfidf_train.todense(),columns=vectorizer.get_feature_names())\n",
        "tfidf_test=pd.DataFrame(tfidf_test.todense(),columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fz7E6c6zAQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge the sentiment column from the original training data and the calculated TF-IDF matrix\n",
        "tfidf_train.insert(0,\"target_var\",pd.to_numeric(train_cleaned[1]))\n",
        "tfidf_test.insert(0,\"target_var\",pd.to_numeric(test_cleaned[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAqX3OAAeDQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split training features and labels\n",
        "traindata = tfidf_train.iloc[:, 1:]\n",
        "trainlabel = pd.DataFrame(tfidf_train.iloc[:, 0])\n",
        "\n",
        "# Split testing features and labels\n",
        "testdata = tfidf_test.iloc[:, 1:]\n",
        "testlabel = pd.DataFrame(tfidf_test.iloc[:, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1UTJCBL3Xpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class for logistic regression\n",
        "class LogisticRegression:\n",
        "    def __init__(self):\n",
        "        # Initialize learning rate and weights\n",
        "        self.lr = 0.1\n",
        "        self.theta = np.zeros(shape=(tfidf_train.shape[1]-1, 1))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "       \n",
        "      \n",
        "        # Iterate and learn the parameters\n",
        "        for i in range(1000):\n",
        "            z = np.dot(X, self.theta)\n",
        "            y_pred = self.sigmoid(z)\n",
        "            y_pred = y_pred.reshape(y.shape)\n",
        "            gradient = np.dot(X.T, (y_pred - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "\n",
        "\n",
        "    # Get the probabilities for predictions\n",
        "    def predict_prob(self, X):\n",
        "       # intercept = np.ones((X.shape[0], 1))\n",
        "        return self.sigmoid(np.dot(X, self.theta))\n",
        "\n",
        "    # Determine the label based on probabilities\n",
        "    def predict(self, X):\n",
        "        predictedLabels = np.array([])\n",
        "        pred_probs = self.predict_prob(X)\n",
        "\n",
        "        for prob in pred_probs:\n",
        "            # The avg probability for 0 and 1 in the training data is around 0.5 so we set the threshold for classification at this mean\n",
        "            if prob >= 0.55:\n",
        "                predictedLabels = np.append(predictedLabels, 1)\n",
        "            else:\n",
        "                predictedLabels = np.append(predictedLabels, 0)\n",
        "\n",
        "        return predictedLabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPmuD8vY3kWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Array for storing accuracy scores\n",
        "scores = np.array([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl__-antgboU",
        "colab_type": "code",
        "outputId": "075d234a-52fc-412e-a964-aadeaab22eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainlabel.to_numpy().shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7O5R49_3pbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize 10-fold cross validation\n",
        "kf = KFold(n_splits=10)\n",
        "\n",
        "# Iterate over folds and call the classifier\n",
        "for train_index, test_index in kf.split(traindata):\n",
        "    x_train, x_test = traindata.iloc[train_index], traindata.iloc[test_index]\n",
        "    y_train, y_test = trainlabel.iloc[train_index], trainlabel.iloc[test_index]\n",
        "    logReg = LogisticRegression()\n",
        "    logReg.fit(x_train.to_numpy(), y_train.to_numpy())\n",
        "    pred = logReg.predict(x_test)\n",
        "\n",
        "    scores = np.append(scores, accuracy_score(y_test, pred))\n",
        "\n",
        "print(\"Accuracy over 10-fold cross validations are: \", scores)\n",
        "print(\"Mean accuracy: \", scores.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMHiW721O0k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f755834f-2ce5-4335-df2c-e535ae71539d"
      },
      "source": [
        "log=LogisticRegression()\n",
        "test_predicted = log.predict(testdata)\n",
        "#Accuracy on test data\n",
        "accuracy_test = accuracy_score(testlabel, test_predicted)\n",
        "accuracy_test"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLotRfVO87y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f50080fc-7a67-4ff1-d1ad-fa7632b5d8b3"
      },
      "source": [
        "# Create confusion matrix\n",
        "confusionMatrix = pd.DataFrame(data = confusion_matrix(testlabel, (test_predicted >= 0.5) .astype(int)), columns=[\"0\", \"1\"], index = [\"0\", \"1\"])\n",
        "print(confusionMatrix)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0     1\n",
            "0  0  4006\n",
            "1  0  5994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyS9dmtUPBOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a0085ee-2b1c-4f00-de69-94abd283b587"
      },
      "source": [
        "# Precision = TP/(TP + FP)\n",
        "precision = round((confusionMatrix.iloc[1, 1] / (confusionMatrix.iloc[1, 1] + confusionMatrix.iloc[0, 1])) * 100, 2)\n",
        "print(\"Precision: \", precision)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  59.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NNgSv57PCcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85b11e9c-158a-43ed-f33f-4eaeaf16842f"
      },
      "source": [
        "# Recall = TP/(TP + FN)\n",
        "recall = round((confusionMatrix.iloc[1, 1] / (confusionMatrix.iloc[1, 1] + confusionMatrix.iloc[1, 0])) * 100, 2)\n",
        "print(\"Recall: \", recall)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall:  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}